{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d602e75d",
   "metadata": {},
   "source": [
    "# 0. Import Libraries/Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d97e5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import glob\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "from tensorflow.keras.layers import (LSTM, Dense, Concatenate, Attention, Dropout, Softmax,\n",
    "                                     Input, Flatten, Activation, Bidirectional, Permute, multiply, \n",
    "                                     ConvLSTM2D, MaxPooling3D, TimeDistributed, Conv2D, MaxPooling2D)\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# disable some of the tf/keras training warnings \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"3\"\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "tf.autograph.set_verbosity(1)\n",
    "\n",
    "# suppress untraced functions warning\n",
    "import absl.logging\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f470e2",
   "metadata": {},
   "source": [
    "# 1. Keypoints using MP Pose\n",
    "- Load MediaPipe's (MP) Pose model for full-body keypoint detection\n",
    "- Initialize MP's drawing utilities to visualize the landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20cde117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained pose estimation model from Google Mediapipe\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Supported Mediapipe visualization tools\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "716e9f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    \"\"\"\n",
    "    This function detects human pose estimation keypoints from webcam footage\n",
    "    \n",
    "    \"\"\"\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9bd7ba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    \"\"\"\n",
    "    This function draws keypoints and landmarks detected by the human pose estimation model\n",
    "    \n",
    "    \"\"\"\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2), \n",
    "                                mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2) \n",
    "                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad2e7a0",
   "metadata": {},
   "source": [
    "# 2. Extract Frames from Videos\n",
    "- Load each video file using OpenCV\n",
    "- Sample frames at a consistent frame rate (e.g. 15-30 fps) for temporal consistency\n",
    "- Resize frames to a fixed size (e.g. 224x224) to reduce computation (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7ba5c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, frame_rate=15):\n",
    "    \"\"\"\n",
    "    This function extracts and resizes frames from a video at a specified sampling rate\n",
    "\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []     # Define frames as an empty list\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)     # Extract the fps from the video\n",
    "    frame_interval = int(fps / frame_rate)\n",
    "    count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        # Read a frame from the video file    \n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break     # Break loop if the video is finished\n",
    "\n",
    "        # If the modulo is 0 btwn the count and the frame_interval --> resize the frame\n",
    "        if count % frame_interval == 0:\n",
    "            frame = cv2.resize(frame, (224, 224))\n",
    "            frames.append(frame)     # Adds the frame to the frames list (at the end)\n",
    "\n",
    "        count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fb32f3",
   "metadata": {},
   "source": [
    "# 3. Extract Keypoints\n",
    "- Extract landmark coordinates from each frame using MediaPipe\n",
    "- Convert landmarks into feature vectors (i.e. angles, distances, or normalized coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bdc9f7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pose_features(frames):\n",
    "    \"\"\"\n",
    "    This function extracts the landmarks from each frame and converts them into feature vectors\n",
    "\n",
    "    \"\"\"\n",
    "    pose = mp_pose.Pose(static_image_mode=False)\n",
    "    features = []\n",
    "\n",
    "    for frame in frames:\n",
    "        results = pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        if results.pose_landmarks:\n",
    "            \n",
    "            # Extract landmark positions / compute angles/distances here\n",
    "            landmarks = [(lm.x, lm.y, lm.z) for lm in results.pose_landmarks.landmark]\n",
    "            features.append(landmarks)\n",
    "        else:\n",
    "            features.append(None)\n",
    "        \n",
    "    pose.close()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b907e8",
   "metadata": {},
   "source": [
    "# 4. Load Data from Directory\n",
    "- Define the path to the video dataset (video_dir)\n",
    "- Load videos from a directory (folder), and create labels for the videos based on the sub directory (the sub folders labelled \"bicep curl\", \"squat\" etc.)\n",
    "- For each sequence (including pose features per frame) apply a label, creating LSTM-ready sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "49df261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_label_names(video_dir):\n",
    "    \"\"\"\n",
    "    This function extracts labels from the folder structure of a video dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    video_paths = glob.glob(os.path.join(video_dir, '**','*.mp4'), recursive=True)\n",
    "    label_names = []\n",
    "    \n",
    "    # Infer label from the folder name\n",
    "    for video_path in video_paths:\n",
    "        #filename = os.path.basename(video_path)\n",
    "        label = os.path.basename(os.path.dirname(video_path))\n",
    "        label_names.append(label)\n",
    "\n",
    "    return label_names    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ab9bd39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_videos_from_directory(video_dir, frame_rate=15, sequence_length=30):\n",
    "    \"\"\"\n",
    "    This function converts labeled videos into pose-based training sequences for an ML model\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    video_paths = glob.glob(os.path.join(video_dir, '**','*.mp4'), recursive=True)\n",
    "    all_sequences = []\n",
    "    all_labels = []\n",
    "    label_names = []\n",
    "\n",
    "    # Infer label from the folder name\n",
    "    for video_path in video_paths:\n",
    "        label = os.path.basename(os.path.dirname(video_path))\n",
    "        label_names.append(label)        \n",
    "\n",
    "    # Create a label map (map numerical labels to class names)\n",
    "    label_map = {label: idx for idx, label in enumerate(sorted(set(label_names)))}\n",
    "\n",
    "    # Read videos and extract frames\n",
    "    for video_path in video_paths:\n",
    "        label = os.path.basename(os.path.dirname(video_path))\n",
    "        label_idx = label_map[label]\n",
    "\n",
    "        # Extract frames and features\n",
    "        print(f\"Processing: {video_path}\")\n",
    "        frames = extract_frames(video_path, frame_rate=frame_rate)\n",
    "        if not frames:     # Print a warning if no frames were extracted\n",
    "            print(\"No frames extracted!\")\n",
    "            continue\n",
    "        \n",
    "        pose_features = extract_pose_features(frames)\n",
    "        pose_features = [f for f in pose_features if f is not None] # Remove frames with missing data\n",
    "\n",
    "        print(f\" --> {len(pose_features)} pose features.\")\n",
    "        \n",
    "        if len(pose_features) < sequence_length:\n",
    "            print(\"Warning: Not enough valid frames for sequence\")\n",
    "            continue\n",
    "\n",
    "        # Convert to np.array for easier slicing\n",
    "        pose_features = np.array(pose_features)\n",
    "        \n",
    "        # Create sliding window sequences\n",
    "        for i in range(len(pose_features) - sequence_length + 1):\n",
    "            sequence = pose_features[i : i + sequence_length]\n",
    "            all_sequences.append(sequence)\n",
    "            all_labels.append(label_idx)\n",
    "    \n",
    "    # Testing/Debugging - check that number of sequences and labels is the same\n",
    "    print(\"Number of sequences: \", len(all_sequences))\n",
    "    print(\"Number of labels: \", len(all_labels))\n",
    "    print(\"Label map: \", label_map)\n",
    "\n",
    "    X = np.array(all_sequences)\n",
    "    y = to_categorical(all_labels).astype(int)\n",
    "\n",
    "    # Returns: X (sequences), y (labels), label_map (dict)\n",
    "    return X, y, label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ddcaecfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: D:\\Programming\\W26_HealthTech_Innovation_Challenge_Guides\\Remote_(At_Home)_Care\\data\\biceps_curl\\12.mp4\n",
      " --> 123 pose features.\n",
      "Processing: D:\\Programming\\W26_HealthTech_Innovation_Challenge_Guides\\Remote_(At_Home)_Care\\data\\biceps_curl\\19.mp4\n",
      " --> 42 pose features.\n",
      "Processing: D:\\Programming\\W26_HealthTech_Innovation_Challenge_Guides\\Remote_(At_Home)_Care\\data\\biceps_curl\\44.mp4\n",
      "No frames extracted!\n",
      "Processing: D:\\Programming\\W26_HealthTech_Innovation_Challenge_Guides\\Remote_(At_Home)_Care\\data\\biceps_curl\\45.mp4\n",
      " --> 421 pose features.\n",
      "Processing: D:\\Programming\\W26_HealthTech_Innovation_Challenge_Guides\\Remote_(At_Home)_Care\\data\\biceps_curl\\47.mp4\n",
      "No frames extracted!\n",
      "Processing: D:\\Programming\\W26_HealthTech_Innovation_Challenge_Guides\\Remote_(At_Home)_Care\\data\\biceps_curl\\55.mp4\n",
      "No frames extracted!\n",
      "Processing: D:\\Programming\\W26_HealthTech_Innovation_Challenge_Guides\\Remote_(At_Home)_Care\\data\\shoulder_press\\1.mp4\n",
      "No frames extracted!\n",
      "Processing: D:\\Programming\\W26_HealthTech_Innovation_Challenge_Guides\\Remote_(At_Home)_Care\\data\\shoulder_press\\10.mp4\n",
      "No frames extracted!\n",
      "Processing: D:\\Programming\\W26_HealthTech_Innovation_Challenge_Guides\\Remote_(At_Home)_Care\\data\\shoulder_press\\14.mp4\n",
      "No frames extracted!\n",
      "Processing: D:\\Programming\\W26_HealthTech_Innovation_Challenge_Guides\\Remote_(At_Home)_Care\\data\\shoulder_press\\15.mp4\n",
      "No frames extracted!\n",
      "Processing: D:\\Programming\\W26_HealthTech_Innovation_Challenge_Guides\\Remote_(At_Home)_Care\\data\\shoulder_press\\17.mp4\n",
      "No frames extracted!\n",
      "Processing: D:\\Programming\\W26_HealthTech_Innovation_Challenge_Guides\\Remote_(At_Home)_Care\\data\\shoulder_press\\3.mp4\n",
      "No frames extracted!\n",
      "Processing: D:\\Programming\\W26_HealthTech_Innovation_Challenge_Guides\\Remote_(At_Home)_Care\\data\\shoulder_press\\5.mp4\n",
      "No frames extracted!\n",
      "Processing: D:\\Programming\\W26_HealthTech_Innovation_Challenge_Guides\\Remote_(At_Home)_Care\\data\\shoulder_press\\7.mp4\n",
      "No frames extracted!\n",
      "Processing: D:\\Programming\\W26_HealthTech_Innovation_Challenge_Guides\\Remote_(At_Home)_Care\\data\\shoulder_press\\9.mp4\n",
      "No frames extracted!\n",
      "Processing: D:\\Programming\\W26_HealthTech_Innovation_Challenge_Guides\\Remote_(At_Home)_Care\\data\\squat\\13.mp4\n",
      "No frames extracted!\n",
      "Processing: D:\\Programming\\W26_HealthTech_Innovation_Challenge_Guides\\Remote_(At_Home)_Care\\data\\squat\\14.mp4\n",
      "No frames extracted!\n",
      "Processing: D:\\Programming\\W26_HealthTech_Innovation_Challenge_Guides\\Remote_(At_Home)_Care\\data\\squat\\17.mp4\n",
      " --> 110 pose features.\n",
      "Processing: D:\\Programming\\W26_HealthTech_Innovation_Challenge_Guides\\Remote_(At_Home)_Care\\data\\squat\\18.mp4\n",
      " --> 93 pose features.\n",
      "Processing: D:\\Programming\\W26_HealthTech_Innovation_Challenge_Guides\\Remote_(At_Home)_Care\\data\\squat\\19.mp4\n",
      "No frames extracted!\n",
      "Processing: D:\\Programming\\W26_HealthTech_Innovation_Challenge_Guides\\Remote_(At_Home)_Care\\data\\squat\\2.mp4\n",
      "No frames extracted!\n",
      "Processing: D:\\Programming\\W26_HealthTech_Innovation_Challenge_Guides\\Remote_(At_Home)_Care\\data\\squat\\3.mp4\n",
      "No frames extracted!\n",
      "Processing: D:\\Programming\\W26_HealthTech_Innovation_Challenge_Guides\\Remote_(At_Home)_Care\\data\\squat\\5.mp4\n",
      "No frames extracted!\n",
      "Processing: D:\\Programming\\W26_HealthTech_Innovation_Challenge_Guides\\Remote_(At_Home)_Care\\data\\squat\\6.mp4\n",
      "No frames extracted!\n",
      "Number of sequences:  644\n",
      "Number of labels:  644\n",
      "Label map:  {'biceps_curl': 0, 'shoulder_press': 1, 'squat': 2}\n",
      "(644, 30, 33, 3) (644, 3)\n"
     ]
    }
   ],
   "source": [
    "# file path - where the videos are located\n",
    "# copy the path to the main data folder, NOT the sub-folders (bicep curl, squat, etc.)\n",
    "video_dir = 'D:\\Programming\\W26_HealthTech_Innovation_Challenge_Guides\\Remote_(At_Home)_Care\\data'\n",
    "sequence_length = 30\n",
    "\n",
    "X, y, label_map = process_videos_from_directory(video_dir, frame_rate=15, sequence_length=sequence_length)\n",
    "\n",
    "# Make sure the first dimensions of arrays match\n",
    "print(X.shape, y.shape)    # X.shape: # of samples (sequences), time_steps (sequence_length), features per timestep (33 landmarks from MP), landmark values (x,y,z)\n",
    "                           # y.shape: encoded labels (# sequences), # of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7622b573",
   "metadata": {},
   "source": [
    "# 5. Split into Training, Validation, and Testing datasets\n",
    "- Split: 75% training, 15% validation, 10% test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5ac49993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(579, 30, 33, 3) (579, 3)\n"
     ]
    }
   ],
   "source": [
    "# Split 10% of data for the testing dataset\n",
    "    # random_state= --> ensures reproducibility, meaning you'll get the same split every time you run the code\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=1)\n",
    "\n",
    "# Check the number of samples in the training dataset\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# Split about 16% of the remaining training data into validation (~ 15% of the total dataset)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=15/90, random_state=2)\n",
    "\n",
    "# Adjust shape of samples to fit in the LSTM [batch_size (num_sequences), time_steps (sequence_length), features_per_timestep (landmarks * coordinates per landmark)]\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 33 * 3)\n",
    "X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 33 * 3)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 33 * 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53ae03d",
   "metadata": {},
   "source": [
    "# 6. Build and Train Neural Networks\n",
    "- Configuring model training: \n",
    "    - EarlyStopping - stops training if loss stops improving\n",
    "    - ReduceLROnPlateau - reduce learning rate is loss stops improving\n",
    "    - ModelCheckpoint - saves the best model w/ the least loss\n",
    "    - Implementing optimizers, and hyperparameters\n",
    "- Sequential LSTM Model Construction\n",
    "- Attention-based bidirectinal LSTM (basically an enhanced version of the sequential LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "912f3153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks to be used during neural network training \n",
    "es_callback = EarlyStopping(monitor='val_loss', min_delta=5e-4, patience=10, verbose=0, mode='min')\n",
    "lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001, verbose=0, mode='min')\n",
    "chkpt_callback = ModelCheckpoint(filepath=os.path.join(video_dir, \".keras\"), monitor='val_loss', verbose=0, save_best_only=True, \n",
    "                                 save_weights_only=False, mode='min', save_freq=1)\n",
    "\n",
    "# Optimizer\n",
    "opt_lstm = Adam(learning_rate=0.001) # for the LSTM model\n",
    "opt_attn = Adam(learning_rate=0.001) # for the LSTM + Attention model\n",
    "\n",
    "# some hyperparamters\n",
    "batch_size = 32     # number samples per training batch\n",
    "max_epochs = 500    # epoch - one full training iteration over the full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ca0cad",
   "metadata": {},
   "source": [
    "## 6a. LSTM\n",
    "- Baseline sequential LSTM model\n",
    "- TensorBoard & Callbacks setup\n",
    "    - For monitoring, debugging, and optimizing model performance\n",
    "- Model Architecture:\n",
    "    - Input layer -> 3 LSTM layers -> 2 Dense layers -> Output layer\n",
    "- Compiles and trains LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "730f987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Tensorboard logging and callbacks\n",
    "NAME = f\"ExerciseRecognition-LSTM-{int(time.time())}\"\n",
    "log_dir = os.path.join(os.getcwd(), 'logs', NAME,'')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "callbacks = [tb_callback, es_callback, lr_callback, chkpt_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ae7595c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(644, 30, 99)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">116,736</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">197,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m116,736\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m394,240\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m197,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">733,059</span> (2.80 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m733,059\u001b[0m (2.80 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">733,059</span> (2.80 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m733,059\u001b[0m (2.80 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# num_input_values: 33 landmarks (from MP) *3 (x, y, z)\n",
    "num_input_values = 33 * 3\n",
    "X = X.reshape(X.shape[0], sequence_length, num_input_values)\n",
    "print(X.shape)\n",
    "\n",
    "# an array of the exercises\n",
    "label_names = extract_label_names(video_dir)\n",
    "\n",
    "lstm = Sequential()\n",
    "lstm.add(Input(shape=(sequence_length, num_input_values)))      # Explicit input\n",
    "lstm.add(LSTM(128, return_sequences=True, activation='relu'))   # Looks at short-term patterns in the motion\n",
    "lstm.add(LSTM(256, return_sequences=True, activation='relu'))   # Looks deeper at more abstract movement sequences\n",
    "lstm.add(LSTM(128, return_sequences=False, activation='relu'))  # Condenses all info to make final decision\n",
    "# Fully connected layers to interpret LSTM output\n",
    "lstm.add(Dense(128, activation='relu'))\n",
    "lstm.add(Dense(64, activation='relu'))\n",
    "# Output a probability for each possible action using 'softmax'\n",
    "lstm.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "print(lstm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8a10e698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 842ms/step - categorical_accuracy: 0.7427 - loss: 0.7624 - val_categorical_accuracy: 0.8557 - val_loss: 0.7261 - learning_rate: 0.0010\n",
      "Epoch 2/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 643ms/step - categorical_accuracy: 0.9004 - loss: 0.4604 - val_categorical_accuracy: 0.9278 - val_loss: 0.3157 - learning_rate: 0.0010\n",
      "Epoch 3/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 756ms/step - categorical_accuracy: 0.9398 - loss: 0.2972 - val_categorical_accuracy: 0.9588 - val_loss: 0.2746 - learning_rate: 0.0010\n",
      "Epoch 4/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - categorical_accuracy: 0.9087 - loss: 0.2729 - val_categorical_accuracy: 0.9588 - val_loss: 0.1280 - learning_rate: 0.0010\n",
      "Epoch 5/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - categorical_accuracy: 0.9523 - loss: 0.1444 - val_categorical_accuracy: 0.9381 - val_loss: 0.1170 - learning_rate: 0.0010\n",
      "Epoch 6/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 931ms/step - categorical_accuracy: 0.9689 - loss: 0.0716 - val_categorical_accuracy: 0.9588 - val_loss: 0.0846 - learning_rate: 0.0010\n",
      "Epoch 7/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 856ms/step - categorical_accuracy: 0.9730 - loss: 0.0487 - val_categorical_accuracy: 0.9485 - val_loss: 0.0719 - learning_rate: 0.0010\n",
      "Epoch 8/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1s/step - categorical_accuracy: 0.9938 - loss: 0.0253 - val_categorical_accuracy: 0.9588 - val_loss: 0.1939 - learning_rate: 0.0010\n",
      "Epoch 9/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 909ms/step - categorical_accuracy: 0.8216 - loss: 3.8582 - val_categorical_accuracy: 0.8041 - val_loss: 0.8366 - learning_rate: 0.0010\n",
      "Epoch 10/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 840ms/step - categorical_accuracy: 0.8942 - loss: 0.3675 - val_categorical_accuracy: 0.9278 - val_loss: 0.2093 - learning_rate: 0.0010\n",
      "Epoch 11/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - categorical_accuracy: 0.8361 - loss: 0.4869 - val_categorical_accuracy: 0.8660 - val_loss: 0.2841 - learning_rate: 0.0010\n",
      "Epoch 12/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 935ms/step - categorical_accuracy: 0.7946 - loss: 2.3157 - val_categorical_accuracy: 0.8041 - val_loss: 0.5251 - learning_rate: 0.0010\n",
      "Epoch 13/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 861ms/step - categorical_accuracy: 0.7718 - loss: 0.4361 - val_categorical_accuracy: 0.8763 - val_loss: 0.3278 - learning_rate: 2.0000e-04\n",
      "Epoch 14/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 875ms/step - categorical_accuracy: 0.9087 - loss: 0.2511 - val_categorical_accuracy: 0.9588 - val_loss: 0.1420 - learning_rate: 2.0000e-04\n",
      "Epoch 15/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - categorical_accuracy: 0.9730 - loss: 0.1021 - val_categorical_accuracy: 0.9691 - val_loss: 0.0649 - learning_rate: 2.0000e-04\n",
      "Epoch 16/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 892ms/step - categorical_accuracy: 0.9730 - loss: 0.1012 - val_categorical_accuracy: 0.9175 - val_loss: 0.1675 - learning_rate: 2.0000e-04\n",
      "Epoch 17/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 976ms/step - categorical_accuracy: 0.9689 - loss: 0.0871 - val_categorical_accuracy: 0.9794 - val_loss: 0.0641 - learning_rate: 2.0000e-04\n",
      "Epoch 18/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1s/step - categorical_accuracy: 0.9793 - loss: 0.0716 - val_categorical_accuracy: 1.0000 - val_loss: 0.0338 - learning_rate: 2.0000e-04\n",
      "Epoch 19/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 924ms/step - categorical_accuracy: 0.9979 - loss: 0.0209 - val_categorical_accuracy: 1.0000 - val_loss: 0.0140 - learning_rate: 2.0000e-04\n",
      "Epoch 20/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 961ms/step - categorical_accuracy: 1.0000 - loss: 0.0086 - val_categorical_accuracy: 1.0000 - val_loss: 0.0054 - learning_rate: 2.0000e-04\n",
      "Epoch 21/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 868ms/step - categorical_accuracy: 1.0000 - loss: 0.0037 - val_categorical_accuracy: 1.0000 - val_loss: 0.0026 - learning_rate: 2.0000e-04\n",
      "Epoch 22/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 923ms/step - categorical_accuracy: 1.0000 - loss: 0.0017 - val_categorical_accuracy: 1.0000 - val_loss: 0.0013 - learning_rate: 2.0000e-04\n",
      "Epoch 23/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 815ms/step - categorical_accuracy: 1.0000 - loss: 9.4121e-04 - val_categorical_accuracy: 1.0000 - val_loss: 8.1122e-04 - learning_rate: 2.0000e-04\n",
      "Epoch 24/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 824ms/step - categorical_accuracy: 1.0000 - loss: 6.4074e-04 - val_categorical_accuracy: 1.0000 - val_loss: 4.0250e-04 - learning_rate: 2.0000e-04\n",
      "Epoch 25/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 745ms/step - categorical_accuracy: 1.0000 - loss: 2.2762e-04 - val_categorical_accuracy: 1.0000 - val_loss: 1.2445e-04 - learning_rate: 2.0000e-04\n",
      "Epoch 26/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 503ms/step - categorical_accuracy: 1.0000 - loss: 9.5488e-05 - val_categorical_accuracy: 1.0000 - val_loss: 6.8779e-05 - learning_rate: 2.0000e-04\n",
      "Epoch 27/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 568ms/step - categorical_accuracy: 1.0000 - loss: 6.3583e-05 - val_categorical_accuracy: 1.0000 - val_loss: 4.7597e-05 - learning_rate: 2.0000e-04\n",
      "Epoch 28/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 1s/step - categorical_accuracy: 1.0000 - loss: 4.7858e-05 - val_categorical_accuracy: 1.0000 - val_loss: 3.7289e-05 - learning_rate: 2.0000e-04\n",
      "Epoch 29/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 1s/step - categorical_accuracy: 1.0000 - loss: 3.9108e-05 - val_categorical_accuracy: 1.0000 - val_loss: 3.1279e-05 - learning_rate: 2.0000e-04\n",
      "Epoch 30/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 1s/step - categorical_accuracy: 1.0000 - loss: 3.3753e-05 - val_categorical_accuracy: 1.0000 - val_loss: 2.6892e-05 - learning_rate: 2.0000e-04\n",
      "Epoch 31/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 903ms/step - categorical_accuracy: 1.0000 - loss: 3.0694e-05 - val_categorical_accuracy: 1.0000 - val_loss: 2.6201e-05 - learning_rate: 4.0000e-05\n",
      "Epoch 32/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - categorical_accuracy: 1.0000 - loss: 2.9934e-05 - val_categorical_accuracy: 1.0000 - val_loss: 2.5489e-05 - learning_rate: 4.0000e-05\n",
      "Epoch 33/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 915ms/step - categorical_accuracy: 1.0000 - loss: 2.9107e-05 - val_categorical_accuracy: 1.0000 - val_loss: 2.4565e-05 - learning_rate: 4.0000e-05\n",
      "Epoch 34/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 921ms/step - categorical_accuracy: 1.0000 - loss: 2.8253e-05 - val_categorical_accuracy: 1.0000 - val_loss: 2.3969e-05 - learning_rate: 4.0000e-05\n",
      "Epoch 35/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 908ms/step - categorical_accuracy: 1.0000 - loss: 2.7465e-05 - val_categorical_accuracy: 1.0000 - val_loss: 2.3253e-05 - learning_rate: 4.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1bc975b74c0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.compile(optimizer=opt_lstm, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "lstm.fit(X_train, y_train, batch_size=batch_size, epochs=max_epochs, validation_data=(X_val, y_val), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f58c4d8",
   "metadata": {},
   "source": [
    "## 6b. LSTM + Attention\n",
    "- Attention mechanism (attention_block)\n",
    "- Model Architecture:\n",
    "    - Input layer -> Bi-LSTM layer -> Attention layer -> Dense layer -> Output layer\n",
    "- Compiles & Trains AttnLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c6e12666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Tensorboard logging and callbacks\n",
    "NAME = f\"ExerciseRecognition-AttnLSTM-{int(time.time())}\"\n",
    "log_dir = os.path.join(os.getcwd(), 'logs', NAME,'')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "callbacks = [tb_callback, es_callback, lr_callback, chkpt_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "07591dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_block(inputs, time_steps):\n",
    "    \"\"\"\n",
    "    Attention layer for deep neural network\n",
    "    \n",
    "    \"\"\"\n",
    "    # Attention weights\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Dense(time_steps, activation='softmax')(a)\n",
    "    \n",
    "    # Attention vector\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    \n",
    "    # Luong's multiplicative score\n",
    "    output_attention_mul = multiply([inputs, a_probs], name='attention_mul') \n",
    "    \n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c5c2e2e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">729,088</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ permute (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Permute</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">930</span> │ permute[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_vec       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Permute</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_mul       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)          │                   │            │ attention_vec[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15360</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ attention_mul[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,864,832</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,539</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m99\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │    \u001b[38;5;34m729,088\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ permute (\u001b[38;5;33mPermute\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m30\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m30\u001b[0m)   │        \u001b[38;5;34m930\u001b[0m │ permute[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_vec       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mPermute\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_mul       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ bidirectional[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mMultiply\u001b[0m)          │                   │            │ attention_vec[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15360\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ attention_mul[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │  \u001b[38;5;34m7,864,832\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │      \u001b[38;5;34m1,539\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,596,389</span> (32.79 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,596,389\u001b[0m (32.79 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,596,389</span> (32.79 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,596,389\u001b[0m (32.79 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_UNITS = 256\n",
    "\n",
    "# Input\n",
    "inputs = Input(shape=(sequence_length, num_input_values))\n",
    "\n",
    "# Bi-LSTM: looks at a sequence forward and backward\n",
    "lstm_out = Bidirectional(LSTM(HIDDEN_UNITS, return_sequences=True))(inputs)\n",
    "\n",
    "# Attention\n",
    "attention_mul = attention_block(lstm_out, sequence_length)\n",
    "attention_mul = Flatten()(attention_mul)\n",
    "\n",
    "# Fully Connected Layer\n",
    "x = Dense(2*HIDDEN_UNITS, activation='relu')(attention_mul)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# Output\n",
    "x = Dense(y.shape[1], activation='softmax')(x)\n",
    "\n",
    "# Bring it all together\n",
    "AttnLSTM = Model(inputs=[inputs], outputs=x)\n",
    "print(AttnLSTM.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cf2f988d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 2s/step - categorical_accuracy: 0.7614 - loss: 0.5629 - val_categorical_accuracy: 0.8557 - val_loss: 0.3407 - learning_rate: 0.0010\n",
      "Epoch 2/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - categorical_accuracy: 0.9149 - loss: 0.2825 - val_categorical_accuracy: 0.9381 - val_loss: 0.2024 - learning_rate: 0.0010\n",
      "Epoch 3/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - categorical_accuracy: 0.9544 - loss: 0.1111 - val_categorical_accuracy: 0.9381 - val_loss: 0.3218 - learning_rate: 0.0010\n",
      "Epoch 4/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 2s/step - categorical_accuracy: 0.9502 - loss: 0.1581 - val_categorical_accuracy: 1.0000 - val_loss: 0.0858 - learning_rate: 0.0010\n",
      "Epoch 5/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 4s/step - categorical_accuracy: 0.9813 - loss: 0.0636 - val_categorical_accuracy: 1.0000 - val_loss: 0.0372 - learning_rate: 0.0010\n",
      "Epoch 6/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 0.0210 - val_categorical_accuracy: 0.9897 - val_loss: 0.0316 - learning_rate: 2.0000e-04\n",
      "Epoch 7/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 0.0160 - val_categorical_accuracy: 1.0000 - val_loss: 0.0237 - learning_rate: 2.0000e-04\n",
      "Epoch 8/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 0.0117 - val_categorical_accuracy: 1.0000 - val_loss: 0.0171 - learning_rate: 2.0000e-04\n",
      "Epoch 9/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 3s/step - categorical_accuracy: 0.9938 - loss: 0.0154 - val_categorical_accuracy: 0.9794 - val_loss: 0.0356 - learning_rate: 2.0000e-04\n",
      "Epoch 10/500\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 3s/step - categorical_accuracy: 0.9959 - loss: 0.0143 - val_categorical_accuracy: 1.0000 - val_loss: 0.0084 - learning_rate: 2.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1bcb8b6d8d0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# categorical crossentropy: measures difference btwn predicted probability distribution and actual (true) distribution of classes\n",
    "# aka softmax loss or log loss\n",
    "\n",
    "AttnLSTM.compile(optimizer=opt_attn, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "AttnLSTM.fit(X_train, y_train, batch_size=batch_size, epochs=max_epochs, validation_data=(X_val, y_val), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b89f67cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model map\n",
    "models = {\n",
    "    'LSTM': lstm, \n",
    "    'LSTM_Attention_128HUs': AttnLSTM, \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a928f612",
   "metadata": {},
   "source": [
    "# 7. Save & Load Weights\n",
    "- Save the models (architecture + weights + optimizer state) \n",
    "- Reload the models' trained weights into rebuilt model objects\n",
    "    - Useful for reusing models after training them once, and to avoid retraining when restarting your script/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0a7647ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Weights\n",
    "for model_name, model in models.items():\n",
    "    save_dir = os.path.join(os.getcwd(), f\"{model_name}.h5\")\n",
    "    model.save(save_dir)        # saves the full model, not just weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ed0114a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Weights\n",
    "# Run model rebuild before doing this\n",
    "for model_name, model in models.items():\n",
    "    load_dir = os.path.join(os.getcwd(), f\"{model_name}.h5\")\n",
    "    model.load_weights(load_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7747c6",
   "metadata": {},
   "source": [
    "# 8. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2101a592",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models.values():\n",
    "    res = model.predict(X_test, verbose=0)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b36c98e",
   "metadata": {},
   "source": [
    "# 9. Evaluations using Confusion Matrix and Accuracy\n",
    "- This helps to understand how well the model is performing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ecf242d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store overall evaluation results\n",
    "eval_results = {}\n",
    "eval_results['confusion matrix'] = None            # Placeholder for overall confusion matrix\n",
    "eval_results['accuracy'] = None                    # Placeholder for overall accuracy\n",
    "eval_results['precision'] = None                   # Placeholder for overall precision\n",
    "eval_results['recall'] = None                      # Placeholder for overall recall\n",
    "eval_results['f1 score'] = None                    # Placeholder for overall F1 score\n",
    "\n",
    "# Dictionaries to store evaluation metrics per model\n",
    "confusion_matrices = {}                            # Holds the confusion matrix for each model\n",
    "classification_accuracies = {}                     # Holds the accuracy score for each model\n",
    "precisions = {}                                    # Hold the precision score\n",
    "recalls = {}                                       # Hold the recall score\n",
    "f1_scores = {}                                     # Hold the F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d6778f",
   "metadata": {},
   "source": [
    "## 9a. Confusion Matrices\n",
    "- Predicted vs Actual Labels\n",
    "- Shows how many times the model:\n",
    "    - correctly predicted each class (diagonal values)\n",
    "    - confused a class for another (off-diagonal values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fccbb90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM confusion matrix: \n",
      "[[[14  0]\n",
      "  [ 0 51]]\n",
      "\n",
      " [[51  0]\n",
      "  [ 0 14]]]\n",
      "LSTM_Attention_128HUs confusion matrix: \n",
      "[[[14  0]\n",
      "  [ 0 51]]\n",
      "\n",
      " [[51  0]\n",
      "  [ 0 14]]]\n"
     ]
    }
   ],
   "source": [
    "# Loop through each model (lstm and AttnLSTM)\n",
    "for model_name, model in models.items():\n",
    "    # Predict labels for test data\n",
    "    yhat = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Get list of classification predictions\n",
    "    ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "    yhat = np.argmax(yhat, axis=1).tolist()\n",
    "    \n",
    "    # Confusion matrix\n",
    "    confusion_matrices[model_name] = multilabel_confusion_matrix(ytrue, yhat)\n",
    "    print(f\"{model_name} confusion matrix: {os.linesep}{confusion_matrices[model_name]}\")\n",
    "\n",
    "# Collect/store results \n",
    "eval_results['confusion matrix'] = confusion_matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76c6dc5",
   "metadata": {},
   "source": [
    "## 9b. Accuracy\n",
    "- The overall percentage of correct predictions\n",
    "- Accuracy = (correct predictions) / (total predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e36146f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM classification accuracy = 100.0%\n",
      "LSTM_Attention_128HUs classification accuracy = 100.0%\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in models.items():\n",
    "    # Predict labels for test data\n",
    "    yhat = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Get list of classification predictions\n",
    "    ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "    yhat = np.argmax(yhat, axis=1).tolist()\n",
    "    \n",
    "    # Model accuracy\n",
    "    classification_accuracies[model_name] = accuracy_score(ytrue, yhat)    \n",
    "    print(f\"{model_name} classification accuracy = {round(classification_accuracies[model_name]*100,3)}%\")\n",
    "\n",
    "# Collect results \n",
    "eval_results['accuracy'] = classification_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33efa73a",
   "metadata": {},
   "source": [
    "## 9c. Precision, Recall, and F1 Score\n",
    "- Precision - out of all times the model predicted a class, how often was it correct?\n",
    "    - Precision = (True positives) / (True positives + False positives)\n",
    "- Recall - out of all times a class actually occurred, how often did the model catch it?\n",
    "    - Recall = (True positives) / (True positives + False negatives)\n",
    "- F1 Score - a score that balances both Precision and Recall (the harmonic mean of the 2)\n",
    "    - F1 Score = 2 * (Precision * Recall) / (Precision + Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "35067c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['biceps_curl', 'shoulder_press', 'squat']\n",
      "LSTM weighted average precision = 1.0\n",
      "LSTM weighted average recall = 1.0\n",
      "LSTM weighted average f1-score = 1.0\n",
      "\n",
      "['biceps_curl', 'shoulder_press', 'squat']\n",
      "LSTM_Attention_128HUs weighted average precision = 1.0\n",
      "LSTM_Attention_128HUs weighted average recall = 1.0\n",
      "LSTM_Attention_128HUs weighted average f1-score = 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in models.items():\n",
    "    # Predict labels for test data\n",
    "    yhat = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Get list of classification predictions\n",
    "    ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "    yhat = np.argmax(yhat, axis=1).tolist()\n",
    "\n",
    "    label_names = [label for label, idx in sorted(label_map.items(), key=lambda x:x[1])]\n",
    "    print(label_names)\n",
    "    \n",
    "    labels = [idx for label, idx in sorted(label_map.items(), key=lambda x: x[1])]\n",
    "\n",
    "    # Precision, recall, and f1 score\n",
    "    report = classification_report(ytrue, yhat, labels=labels, target_names=label_names, output_dict=True, zero_division=0)\n",
    "    \n",
    "    precisions[model_name] = report['weighted avg']['precision']\n",
    "    recalls[model_name] = report['weighted avg']['recall']\n",
    "    f1_scores[model_name] = report['weighted avg']['f1-score'] \n",
    "   \n",
    "    print(f\"{model_name} weighted average precision = {round(precisions[model_name],3)}\")\n",
    "    print(f\"{model_name} weighted average recall = {round(recalls[model_name],3)}\")\n",
    "    print(f\"{model_name} weighted average f1-score = {round(f1_scores[model_name],3)}\\n\")\n",
    "\n",
    "# Collect results \n",
    "eval_results['precision'] = precisions\n",
    "eval_results['recall'] = recalls\n",
    "eval_results['f1 score'] = f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d39476",
   "metadata": {},
   "source": [
    "# 10. Choose Model to Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d72d0605",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttnLSTM          # you can also choose to test the baseline model (\"lstm\")\n",
    "model_name = 'AttnLSTM'\n",
    "\n",
    "# Now the ML model is ready to be used in an application!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (REMOTE_CARE)",
   "language": "python",
   "name": "remote_at_home_care"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
